---
output: 
  html_document:
    toc: yes
    toc_float: yes
bibliography: [book.bib, packages.bib, article.bib]
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SPC - Statistical Process Control

Bamako Lightening is a company that manufactures lamps. The weight of each lamp is critical to the quality of the product. The Production Operator monitors the production process using xbar and R-charts. Samples are taken of six lamps every hour and their means and ranges plotted on control charts. 

```{r}
library(readr)
library(dplyr)
```

Bamako Lightening is a company that manufactures lamps. The weight of each lamp is critical to the quality of the product. The Production Operator monitors the production process using xbar and R-charts. Samples are taken of six lamps every hour and their means and ranges plotted on control charts. Data is available representing samples taken a period of 25 hours of production.

Start by loading a csv file from our github repository into our R session:
```{r}
bamako <- readr::read_csv("data/Bamako.csv")
```


```{r, eval = FALSE}
bamako <- readr::read_csv(
  file = "https://raw.githubusercontent.com/J-Ramalho/J-Ramalho.github.io/master/data/Bamako.csv")
```
The message shows that the loading has worked correctly as all columns have been parsed with type double.
Looking at the first five lines to confirm and assess the quality of our data for further processing. 
```{r}
head(bamako)
```

In this table each line corresponds to a sampling hour and each column corresponds to a sample number. 

*xbar and  R charts*

The package {qcc} has been selected to create the control charts.
```{r}
library(qcc)
```
The chart ploting function qcc() already considers that each line corresponds to a sampling event and requires as argument a table containing only the sampling values. For that the Hour column has to be removed.

```{r}
bamako_clean <- bamako %>% dplyr::select(-Hour)
```

Before entering regular production it is recommended to run a "calibration run". The calibration run is used to calculate the control limits. Using the first 10 samples a default Shewart xbar chart can be obtained as follows:
```{r}
qcc(bamako_clean[1:10, ], 
    type = "xbar", 
    title = "Lamp weight \n xbar chart", 
    xlab = "Sample group")

qcc(bamako_clean[1:10, ], 
    type = "R", 
    title = "Lamp weight \n R chart",
    xlab = "Sample group")
```

Afterwards, the production samples can  be assessed against those limits and the control chart rules can be verified. In this example the shewhart rules are used.
```{r}
qcc(data = bamako_clean[1:10, ],
    newdata = bamako_clean[11:25,],
    type = "xbar", 
    title = "Lamp weight \n xbar chart", 
    xlab = "Sample group")

qcc(data = bamako_clean[1:10, ],
    newdata = bamako_clean[11:25,], 
    type = "R", 
    title = "Lamp weight \n R chart",
    xlab = "Sample group")
```


Adapted from [@Bass2007]

# DOE - Simple comparative experiments

[@Montgomery2012], Chapter 2

*The Portlant cement example*

Loading data
```{r}
# To be used in examples with each variable in a different column
cement <- read.csv("data/2 cement.csv")
# To be used in examples with all variables in one column with additional factor column
cement_factor <- read.csv("data/2 cement_factor.csv")
names(cement_factor) <- c("treatment", "y")
```

Normality check  
t test assumptions
* independent populations
* normally distributed
* equal variance
* random sampling

```{r}
library(ggplot2)
```


```{r}
ggplot(cement_factor, aes(x = y, fill = treatment)) +
  geom_histogram()

ggplot(cement_factor, aes(x = y, fill = treatment)) +
  geom_density(alpha = 0.3)
```

My take away is that the populations are too small to plot histograms (Montgomery recommends >75 observations!)

## Homogeneity of variances test (Levene Test)

You want test samples to see for homogeneity of variance (homoscedasticity)

```{r}
library(car)
```

```{r}
leveneTest(y ~ treatment, data = cement_factor)
```

Pr > 0.05 thus there is homogeity of the variances (they do not differ significantly).

```{r}
library(stats)
```


```{r}
# perform t-test
t.test(x = cement$Modified, 
       y = cement$Unmodified,
       var.equal = TRUE)
```
P < 0.05 thus the means differ significantly 
The null hypothesis is rejected (and would be rejected at any level of significance of alpha > 0.0422).

Furthemore the mean difference is estimated with 95% confidence, to be between -0.55 and -0.01 (to be noted that zero is obviously not included in this interval).

*Sample size calculation*

```{r}
library(lsr)
library(pwr)
```


```{r}
# Calculate the required sample size for a certain t-test power
cohen_d <- 0.25 / 0.25 # Cohen's effect size = difference of means / sd
# A Cohen's d of 2 means that the averages changed by 2 standard deviations, which is very large.
pwr.t.test(d = cohen_d, power = 0.95)

# By comparison, calculate Cohen's d for the dataset
cohensD(x = cement$Modified, y = cement$Unmodified)
```
 In this example if we wanted to detect a significant difference of at least 0.25 in the means with a probability of at least 95% (Power of 0.95)  we would need to use 8 (7.6) samples of each.
 
see also [@Broc2016] for further details on the statistical tests presented here.

*t-test paired*
 
```{r}
hardness <- read.csv("data/2 hardness.csv")
t.test(x = hardness$Tip1, y = hardness$Tip2, paired = TRUE)
```
p > 0.05 thus the means cannot be considered different (we cannot reject the null hypothesis)
The mean difference is with 95% confidence between -0.96 and 0.76.

Note that because it is paired although there are 20 measurements there are only 9 degrees of freedom (10 times the differences between the measurements, minus 1).
